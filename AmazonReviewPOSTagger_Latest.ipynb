{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W266 Final Project Code\n",
    "# Amazon Product Review Aspect-Based Sentiment\n",
    "## Jennifer Mahle and Joanna Wang (Sections 3 and 1, respectively) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction\n",
    "For our final project, we built a classification system for Amazon product reviews. The system categorizes product reviews into various classes of what the review focuses on, then determines whether the review is positive or negative for a given product trait (ie durability, quality, etc). As a user, star ratings alone might not give enough information about the product, so reading the reviews still is the best way to determine if the product fits the userâ€™s needs. The challenge is, sometimes there can be hundreds of reviews for a product and users cannot spend time reading all of them.  So we want to provide this classification system to reduce the review reading process and help the users to find what they need. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis\n",
    "\n",
    "In this section, we load, clean, and explore the data. We are using Amazon product reviews for electronics from the website https://nijianmo.github.io/amazon/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages \n",
    "# Importing libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3acc6cad80db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Dropout\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /Users/joannawang/Library/Python/3.7/lib/python/site-packages (2.3.1)\n",
      "Requirement already satisfied: six>=1.9.0 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/site-packages (from keras) (1.12.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /Users/joannawang/Library/Python/3.7/lib/python/site-packages (from keras) (1.0.8)\n",
      "Requirement already satisfied: scipy>=0.14 in /Users/joannawang/Library/Python/3.7/lib/python/site-packages (from keras) (1.4.1)\n",
      "Requirement already satisfied: h5py in /Users/joannawang/Library/Python/3.7/lib/python/site-packages (from keras) (2.10.0)\n",
      "Requirement already satisfied: pyyaml in /Users/joannawang/Library/Python/3.7/lib/python/site-packages (from keras) (5.3.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /Users/joannawang/Library/Python/3.7/lib/python/site-packages (from keras) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /Users/joannawang/Library/Python/3.7/lib/python/site-packages (from keras) (1.18.1)\n",
      "\u001b[33mYou are using pip version 19.0.3, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --user keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DON'T NEED TO RUN THIS PART FOR NOW. RUN THE NEXT CELL TO LOAD DATA\n",
    "#####################################################################\n",
    "dataset = \"Electronics_5.json\"\n",
    "\n",
    "if os.path.isfile(dataset):\n",
    "    df = pd.read_json(\"Electronics_5.json\", lines=True)\n",
    "else:\n",
    "    url = r\"http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Electronics_5.json.gz\"\n",
    "    df = pd.read_json(url, compression='gzip', lines=True)\n",
    "\n",
    "display(df.tail(10))\n",
    "df.shape\n",
    "print(df.info())\n",
    "df_mini = df[(df.asin == \"B01HJCN1EI\") | (df.asin == \"B01HJH42KU\") | \n",
    "                            (df.asin == \"B01HJH40WU\") | (df.asin == \"B01HJF704M\") | \n",
    "                           (df.asin == \"B01HJCN5GC\") | (df.asin == \"B01HJCN5TO\") |\n",
    "                           (df.asin == \"B01HJDNL60\") | (df.asin == \"B01HJDR9DQ\") |\n",
    "                           (df.asin == \"B01HJFFHTC\") | (df.asin == \"B01HJCN1EI\")]\n",
    "df_mini.shape\n",
    "df_mini.to_csv('/home/wangjia/datasci-w266-finalProject/df_mini.csv')\n",
    "######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"df_mini.csv\") \n",
    "display(df.tail(10))\n",
    "df.shape\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove NA review rows\n",
    "df = df.dropna(subset=['reviewText'])\n",
    "#Checking one of the reviews\n",
    "print(df[\"reviewText\"].iloc[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#set of stopwords in English\n",
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "words_to_keep = set(('not'))\n",
    "stop -= words_to_keep\n",
    "#initialising the snowball stemmer\n",
    "sno = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "#function to clean the word of any html-tags\n",
    "def cleanhtml(sentence):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, ' ', sentence)\n",
    "    return cleantext\n",
    "\n",
    "#function to clean the word of any punctuation or special characters\n",
    "def cleanpunc(sentence): \n",
    "    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
    "    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
    "    return  cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for removing HTML tags , punctuations . Code for removing stopwords . Code for checking if word is not alphanumeric and\n",
    "# also greater than 2 . Code for stemmimg and also to convert them to lowercase letters \n",
    "\n",
    "i=0\n",
    "str1=' '\n",
    "final_string=[]\n",
    "all_positive_words=[] # store words from +ve reviews here\n",
    "all_negative_words=[] # store words from -ve reviews here.\n",
    "s=''\n",
    "for sent in df.reviewText:\n",
    "    filtered_sentence=[]\n",
    "    #print(sent);\n",
    "    sent=cleanhtml(sent) # remove HTMl tags\n",
    "    for w in sent.split():\n",
    "        for cleaned_words in cleanpunc(w).split():\n",
    "            if((cleaned_words.isalpha()) & (len(cleaned_words)>2)):    \n",
    "                if(cleaned_words.lower() not in stop):\n",
    "                    s=(sno.stem(cleaned_words.lower())).encode('utf8')\n",
    "                    filtered_sentence.append(s)\n",
    "                    if (df['reviewText'].values)[i] == 1: \n",
    "                        all_positive_words.append(s) #list of all words used to describe positive reviews\n",
    "                    if(df['reviewText'].values)[i] == 0:\n",
    "                        all_negative_words.append(s) #list of all words used to describe negative reviews reviews\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                continue \n",
    "    \n",
    "    str1 = b\" \".join(filtered_sentence) #final string of cleaned words\n",
    "    \n",
    "    \n",
    "    final_string.append(str1)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding a column of CleanedText which displays the data after pre-processing of the review\n",
    "df['CleanedText']=final_string  \n",
    "df['CleanedText']=df['CleanedText'].str.decode(\"utf-8\")\n",
    "#below the processed review can be seen in the CleanedText Column \n",
    "print('Shape of final',df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After processing the sample review looks like this\n",
    "print(df[\"CleanedText\"].iloc[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting data according to asin in ascending order\n",
    "sorted_data=df.sort_values('asin', axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')\n",
    "\n",
    "#Deduplication of entries\n",
    "final=sorted_data.drop_duplicates(subset={\"reviewerID\",\"reviewerName\",\"reviewText\",\"summary\"}, keep='first', inplace=False)\n",
    "\n",
    "#Removed not verified rows\n",
    "final = final[final.verified != False]\n",
    "\n",
    "#Drop NA and name it to x_train \n",
    "x_train = final.dropna(subset=['reviewText'])\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Encoding using Universal Sentence Encoder\n",
    "\n",
    "In the subsequent code cells, we load the Universal Sentence Encoder (USE), break the data into training and testing data, and apply the USE to the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 uninstall tensorflow-gpu\n",
    "!pip3 uninstall tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove ## from lines starting with ! and run them the first time to install necessary packages \n",
    "\n",
    "##%%capture\n",
    "# Install the Tensorflow 2.0.0 version.\n",
    "!pip3 install --user tensorflow==2.0.0\n",
    "# Install TF-Hub.\n",
    "!pip3 install --user tensorflow-hub\n",
    "!pip3 install --user seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Load the Universal Sentence Encoder's TF Hub module\n",
    "from absl import logging\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "\n",
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/4\", \"https://tfhub.dev/google/universal-sentence-encoder-large/5\"]\n",
    "model = hub.load(module_url)\n",
    "print (\"module %s loaded\" % module_url)\n",
    "def embed(input):\n",
    "  return model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create embeddings on the training data \n",
    "logging.set_verbosity(logging.ERROR)\n",
    "message_embeddings = embed(x_train.CleanedText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training X Shape\", x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_embeddings[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install --user keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Dropout\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "\n",
    "# Initialising the model\n",
    "model_1 = Sequential()\n",
    "\n",
    "# Adding embedding\n",
    "model_1.add(Embedding(len(message_embeddings), embedding_vecor_length, input_length=max_review_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stanford POS Tagger to Find Product Attributes\n",
    "\n",
    "We use the Stanford POS tagger to find the most common nouns used in product reviews for each product ID (ASIN). Then we use the most common nouns as product attributes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m pip install --upgrade pip\n",
    "#!pip install torch\n",
    "#!pip install stanfordnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to install java (unless you already have it installed) \n",
    "# and update the path to where ever it is stored on your computer\n",
    "import os\n",
    "java_path = \"C:/Program Files/Java/jre1.8.0_241/bin/java.exe\"\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "\n",
    "# need to follow instructions to install Stanford POS tagger here: \n",
    "# https://phitchuria.wordpress.com/2018/09/29/python-nltk-using-stanford-pos-tagger-in-nltk-on-windows/\n",
    "from nltk.tag import StanfordPOSTagger\n",
    "from nltk.corpus import stopwords\n",
    "stanford_dir = \"C:\\Stanford\\stanford-postagger-2018-10-16\"\n",
    "modelfile = stanford_dir+\"\\models\\english-bidirectional-distsim.tagger\"\n",
    "jarfile=stanford_dir+\"\\stanford-postagger.jar\"\n",
    "\n",
    "tagger=StanfordPOSTagger(model_filename=modelfile, path_to_jar=jarfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dist={}\n",
    "for i in range(1,len(x_train)): \n",
    "#for i in range(1,10): \n",
    "    tagged_POS = tagger.tag(x_train.reviewText[i].split())\n",
    "    for word,tag in tagged_POS:\n",
    "        if tag == 'NN' or tag == 'NNS':\n",
    "            if word in freq_dist:\n",
    "                freq_dist[word] += 1\n",
    "            else:\n",
    "                freq_dist[word] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "sorted_freq_dist=sorted(freq_dist.items(),key=operator.itemgetter(1))\n",
    "# change into the dictionary since it is easier to approach\n",
    "dict_sorted_freq_dist=dict(sorted_freq_dist)\n",
    "\n",
    "print(dict_sorted_freq_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(freq_dist[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
